{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating log file\n",
    "def create_logger(output_path, cfg_name):\n",
    "    log_file = '{}_{}.log'.format(cfg_name, time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    head = '%(asctime)-15s %(message)s'\n",
    "    logging.basicConfig(filename=os.path.join(output_path, log_file), format=head)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Loading and precessing data\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of model\n",
    "def initialize_parameters(layers):\n",
    "    # Initialize parameters according to different types of layers\n",
    "    new_layers = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        mode = layer['mode'] # 'fc'\n",
    "        if mode == 'fc':\n",
    "            n_now = layer['n_now']\n",
    "            n_prev = layer['n_prev']\n",
    "            layer['W']=(np.random.rand(n_now, n_prev) - 0.5) * 0.2 # random sample in [-0.1, 0.1] \n",
    "            layer['b']=np.zeros((n_now,1))\n",
    "            layer['dW']=np.zeros_like(layer['W'])\n",
    "            layer['db']=np.zeros_like(layer['b'])\n",
    "        else:\n",
    "            print('Wrong layer in [{}]'.format(i))\n",
    "        new_layers.append(layer)\n",
    "            \n",
    "    return new_layers\n",
    "\n",
    "def sigmoid(Z):\n",
    "    # Sigmoid activation function\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    # Backpropogation of sigmoid activation function\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ\n",
    "\n",
    "def relu(Z):\n",
    "    # Relu activation function\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    # Backpropogation of Relu activation function \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    dZ[Z < 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def softmax(Z):\n",
    "    # Softmax activation function\n",
    "    n, m = Z.shape\n",
    "    A = np.exp(Z)\n",
    "    A_sum = np.sum(A, axis = 0)\n",
    "    A_sum = A_sum.reshape(-1, m)\n",
    "    A = A / A_sum\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backward(A, Y):\n",
    "    # Backpropogation of softmax activation function\n",
    "    # loss = - ln a[j] (y[j] = 1, j = {0, ..., n}) \n",
    "    m = A.shape[1]\n",
    "    dZ = (A - Y) / np.float(m)\n",
    "    return dZ\n",
    "\n",
    "def linear_activation_forward(A_prev, layer, activation='relu'):\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    if activation=='sigmoid':\n",
    "        Z, linear_cache=np.dot(W, A_prev)+b, (A_prev, W, b)\n",
    "        A, activation_cache=sigmoid(Z)\n",
    "    elif activation=='relu':\n",
    "        Z, linear_cache=np.dot(W, A_prev)+b, (A_prev, W, b)\n",
    "        A, activation_cache=relu(Z)\n",
    "    else:\n",
    "        Z = np.dot(W, A_prev)+b\n",
    "        A = Z\n",
    "    return A, Z\n",
    "\n",
    "def linear_activation_backward(dA, layer, activation):\n",
    "    # Backward propagatIon module - linear activation backward\n",
    "    A_prev = layer['A_prev']\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    Z = layer['Z']\n",
    "    if activation=='relu':\n",
    "        dZ=relu_backward(dA, Z)\n",
    "    elif activation=='sigmoid':\n",
    "        dZ=sigmoid_backward(dA, Z)\n",
    "    else:\n",
    "        dZ = dA \n",
    "    n, m = dA.shape\n",
    "    dA_prev=np.dot(W.T, dZ)\n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis = 1).reshape(n,1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing THREE layers BP neural networks\n",
    "def forward_propogation(X, layers):\n",
    "    m = X.shape[1]\n",
    "    # -1- fully connected layer\n",
    "    layers[0]['A_prev'] = X\n",
    "    A, Z = linear_activation_forward(X, layers[0], activation='relu')\n",
    "    layers[0]['Z'] = Z\n",
    "    \n",
    "    # -2- fully connected layer\n",
    "    layers[1]['A_prev'] = A\n",
    "    A, Z = linear_activation_forward(A, layers[1], activation='relu')\n",
    "    layers[1]['Z'] = Z\n",
    "\n",
    "    # -3- fully connected layer\n",
    "    layers[2]['A_prev'] = A\n",
    "    _, Z = linear_activation_forward(A, layers[2], activation='none')\n",
    "    layers[2]['Z'] = Z\n",
    "    AL, _ = softmax(Z)\n",
    "\n",
    "    return AL, layers\n",
    "\n",
    "def backward_propogation(AL, Y, layers):\n",
    "    m = Y.shape[1]\n",
    "    # -3- fully connected layer\n",
    "    dZ = softmax_backward(AL, Y)\n",
    "    dA_prev, dW, db = linear_activation_backward(dZ, layers[2], 'none')\n",
    "    layers[2]['dW'] = dW\n",
    "    layers[2]['db'] = db\n",
    "    \n",
    "    # -2- fully connected layer\n",
    "    dA_prev, dW, db = linear_activation_backward(dA_prev, layers[1], 'relu')\n",
    "    layers[1]['dW'] = dW\n",
    "    layers[1]['db'] = db\n",
    "\n",
    "    # -1- fully connected layer\n",
    "    dA_prev, dW, db = linear_activation_backward(dA_prev, layers[0], 'relu')\n",
    "    layers[0]['dW'] = dW\n",
    "    layers[0]['db'] = db\n",
    "\n",
    "    return layers\n",
    "\n",
    "def update_parameters(layers, learning_rate):\n",
    "    num_layer = len(layers)\n",
    "    for i in range(num_layer):\n",
    "        mode = layers[i]['mode'] # 'fc'\n",
    "        if mode == 'fc':\n",
    "            layers[i]['W'] = layers[i]['W'] - learning_rate*layers[i]['dW']\n",
    "            layers[i]['b'] = layers[i]['b'] - learning_rate*layers[i]['db']\n",
    "        else:\n",
    "            print('Wrong layer mode in [{}]'.format(i))\n",
    "\n",
    "    return layers\n",
    "\n",
    "def predict(X_test, Y_test, layers):\n",
    "    m = X_test.shape[1]\n",
    "    n = Y_test.shape[0]\n",
    "    pred = np.zeros((n,m))\n",
    "    pred_count = np.zeros((n,m)) - 1 # for counting accurate predictions \n",
    "    \n",
    "    # forward propagation\n",
    "    AL, _ = forward_propogation(X_test, layers)\n",
    "\n",
    "    # convert prediction to 0/1 form\n",
    "    max_index = np.argmax(AL, axis = 0)\n",
    "    pred[max_index, list(range(m))] = 1\n",
    "    pred_count[max_index, list(range(m))] = 1\n",
    "    \n",
    "    accuracy = np.float(np.sum(pred_count == Y_test)) / m\n",
    "    \n",
    "    return pred, accuracy\n",
    "\n",
    "def compute_accuracy(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    pred_count = np.zeros((n,m)) - 1\n",
    "    \n",
    "    max_index = np.argmax(AL, axis = 0)\n",
    "    pred_count[max_index, list(range(m))] = 1\n",
    "    \n",
    "    accuracy = np.float(np.sum(pred_count == Y)) / m\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    cost = - np.sum(np.log(AL) * Y) / m\n",
    "    cost=np.squeeze(cost)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def train_mini_batch(X_train, Y_train, X_test, Y_test, layers, logger, batch_size=10, num_epoch=1, learning_rate=0.01):\n",
    "    logger.info('------------ Train BPNN with mini batch ------------')\n",
    "    logger.info('Initial weights: FC [-0.1, 0.1], Mapping [-0.1, 0.1]')\n",
    "    logger.info('Initial bias: FC 0')\n",
    "    logger.info('Batch size: {}'.format(batch_size))\n",
    "    logger.info('Learning rate: {}'.format(learning_rate))\n",
    "    \n",
    "    # number of iteration\n",
    "    num_sample=X_train.shape[0]\n",
    "    num_iteration = num_sample // batch_size\n",
    "    index = list(range(num_sample))\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        # random.shuffle(index) # random sampling every epoch\n",
    "        for iteration in range(num_iteration):\n",
    "            batch_start = iteration * batch_size\n",
    "            batch_end = (iteration + 1) * batch_size\n",
    "            if batch_end > num_sample:\n",
    "                batch_end = num_sample\n",
    "            X_train_batch = (X_train[index[batch_start:batch_end]]).T\n",
    "            Y_train_batch = (Y_train[index[batch_start:batch_end]]).T\n",
    "            AL, layers = forward_propogation(X_train_batch, layers)\n",
    "            loss = compute_cost(AL, Y_train_batch)\n",
    "            accuracy = compute_accuracy(AL, Y_train_batch)\n",
    "            layers = backward_propogation(AL, Y_train_batch, layers)\n",
    "            layers = update_parameters(layers, learning_rate)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            if (iteration+1) % 600 == 0:\n",
    "                logger.info('Epoch [{}] Iteration [{}]: loss = {} accuracy = {}'.format(epoch, iteration+1, loss, accuracy))\n",
    "                print('Epoch [{}] Iteration [{}]: loss = {} accuracy = {}'.format(epoch, iteration+1, loss, accuracy))\n",
    "                np.save('data/layers_{}_{}.npy'.format(epoch, iteration+1), layers)\n",
    "\n",
    "        _, accuracy_test = predict(X_test.T, Y_test.T, layers)\n",
    "        pred_train, _ = forward_propogation(X_train[:10000].T, layers)\n",
    "        loss_train = compute_cost(pred_train, Y_train[:10000].T)\n",
    "        accuracy_train = compute_accuracy(pred_train, Y_train[:10000].T)\n",
    "        print('Epoch [{}] average_loss = {} average_accuracy = {}'.format(epoch, np.mean(losses), np.mean(accuracies)))\n",
    "        logger.info('Epoch [{}] average_loss = {} average_accuracy = {}'.format(epoch, np.mean(losses), np.mean(accuracies)))\n",
    "        print('Epoch [{}] train_loss = {} train_accuracy = {}'.format(epoch, loss_train, accuracy_train))\n",
    "        logger.info('Epoch [{}] train_loss = {} train_accuracy = {}'.format(epoch, loss_train, accuracy_train))\n",
    "        print('Epoch [{}] test_accuracy = {}'.format(epoch, accuracy_test))\n",
    "        logger.info('Epoch [{}] test_accuracy = {}'.format(epoch, accuracy_test))\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log file\n",
    "logger = create_logger('output', 'train_log')\n",
    "\n",
    "# load dataset\n",
    "X_train, Y_train = load_mnist('data', 'train')\n",
    "X_test, Y_test = load_mnist('data', 'test')\n",
    "# normalization for data\n",
    "X_train = (X_train / 255.0 - 0.5) * 2 # [-1, 1]\n",
    "X_test = (X_test / 255.0 - 0.5) * 2 # [-1, 1]\n",
    "# transform the label into one-hot form\n",
    "(num_train,) = Y_train.shape\n",
    "Y = np.zeros((num_train, 10))\n",
    "for i in range(num_train):\n",
    "    Y[i, Y_train[i]] = 1\n",
    "Y_train = Y\n",
    "(num_test,) = Y_test.shape\n",
    "Y = np.zeros((num_test, 10))\n",
    "for i in range(num_test):\n",
    "    Y[i, Y_test[i]] = 1\n",
    "Y_test = Y\n",
    "\n",
    "# Construct model of THREE layers\n",
    "layer1={}\n",
    "layer1['mode'] = 'fc'\n",
    "layer1['n_now'] = 300\n",
    "layer1['n_prev'] = 784\n",
    "layer2={}\n",
    "layer2['mode'] = 'fc'\n",
    "layer2['n_now'] = 150\n",
    "layer2['n_prev'] = 300\n",
    "layer3={}\n",
    "layer3['mode'] = 'fc'\n",
    "layer3['n_now'] = 10\n",
    "layer3['n_prev'] = 150\n",
    "construct_layers = [layer1, layer2, layer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 2\n",
    "for index in range(num_experiments):\n",
    "    print('------------------------------------- Experiment {} -------------------------------------'.format(index+1))\n",
    "    logger.info('------------------------------------- Experiment {} -------------------------------------'.format(index+1))\n",
    "\n",
    "    initial_layers_path = 'data/initial_layers_{}.npy'.format(index+1)\n",
    "    if os.path.exists(initial_layers_path):\n",
    "        initial_layers = np.load(initial_layers_path)\n",
    "        print('Load initial parameters from {}'.format(initial_layers_path))\n",
    "        logger.info('Load initial parameters from {}'.format(initial_layers_path))\n",
    "    else:\n",
    "        initial_layers = initialize_parameters(construct_layers)\n",
    "        np.save(initial_layers_path, initial_layers)\n",
    "        print('Initialize layers and save as {}'.format(initial_layers_path))\n",
    "        logger.info('Initialize layers and save as {}'.format(initial_layers_path))\n",
    "    \n",
    "    layers = train_mini_batch(X_train, Y_train, X_test, Y_test, initial_layers, logger, batch_size=10, num_epoch=3, learning_rate=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
